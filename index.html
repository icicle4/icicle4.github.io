<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="写点东西给自己看">
<meta property="og:type" content="website">
<meta property="og:title" content="呓语">
<meta property="og:url" content="http://icicle314.xyz/index.html">
<meta property="og:site_name" content="呓语">
<meta property="og:description" content="写点东西给自己看">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="呓语">
<meta name="twitter:description" content="写点东西给自己看">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://icicle314.xyz/"/>





  <title>呓语</title>
  








</head>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<script src="https://unpkg.com/kotlin-playground@1" data-selector="highlight kotlin"></script>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">呓语</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">发牢骚的地方</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2019/10/02/安装并使用scalabel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/02/安装并使用scalabel/" itemprop="url">安装并使用scalabel</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-02T21:26:10+08:00">
                2019-10-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    虽然salabel提供了非docker的安装方法，但是建议用户直接使用docker，避免麻烦，因此下面只抄了docker安装部分。    </p>
<ol>
<li><p>克隆scalabel代码库</p>
<p><code>git clone https://github.com/ucbdrive/scalabel.git</code></p>
</li>
<li><p>使用docker编译代码</p>
<p><code>cd scalabel</code></p>
<p><code>sudo docker pull scalable/www</code></p>
</li>
<li><p>准备数据文件夹</p>
<p><code>mkdir data</code></p>
<p><code>cp app/config/default_config.yml data/config.yml</code></p>
</li>
<li><p>登陆服务器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it -v "`pwd`/data:/opt/scalabel/data" -p 8686:8686 scalabel/www \</span><br><span class="line">    /opt/scalabel/bin/scalabel --config /opt/scalabel/data/config.yml</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>经过以上步骤之后，访问 localhost:8686 即可建立新的项目。其中标签和属性的自定义可以参看scalabel/examples中的yml文件。</p>
<p>接下来是图片文件的提供，建立项目时需要有item_list.yml文件给出所有图片的路径。该文件可以由scalabel/scripts提供的prepare_data.py文件生成，但是直接使用后可能遇到无法找到图片的错误，这是由于docker的文件访问机制决定的。因此将</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yaml_items = [</span><br><span class="line">        &#123;</span><br><span class="line">            &apos;url&apos;: (os.path.abspath(img) if not args.web_root else</span><br><span class="line">                    join(args.web_root, os.path.basename(img))),</span><br><span class="line">            &apos;videoName&apos;: &apos;&#123;&#125;&apos;.format(</span><br><span class="line">                &apos;-&apos;.join(os.path.split(img)[-1].split(&apos;-&apos;)[:-1]))</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>中的第3行改为<code>&#39;url&#39;: os.path.join(&#39;http://localhost:8686&#39;, os.path.joint(args.tar_dir), os.path.basename(img))</code>。生成的目标文件夹的路径为 scalabel/app/dist/video_name。</p>
<p>此外，需要执行下面的指令将该文件夹拷贝到docker中。</p>
<p><code>sudo docker cp &#39;scalabel/app/dist/video_name&#39; &#39;dockers container number&#39;:/opt/scalabel/app/dist/</code></p>
<p>你可以执行 <code>sudo docker container ls</code> 查看docker的容器编号。</p>
<p>之后你就可以愉快地使用scalabel进行无聊地标注了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2019/08/26/笑话一则/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/26/笑话一则/" itemprop="url">笑话一则</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-26T19:20:20+08:00">
                2019-08-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>丁仪转行去搞AI反腐，鉴定腐败的准确度达到了令人惊讶的99.99999%，同行和资本家都为之叹服。</p>
<p>记者：尊敬的丁院士，您是怎么将AI技术在反腐方面做到如此让人叹服的地步呢？</p>
<p>丁仪扶了扶眼镜框，说：设计网络的时候，我不小心忘了把 class_number + 1 了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2019/08/19/pose-estimation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/19/pose-estimation/" itemprop="url">2D多人姿势确认简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-19T21:47:27+08:00">
                2019-08-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>​        <strong>多人姿势确认</strong> 的定义为从单张图片中推断出预先未知人数的人体关节位置，并确定其归属。2D姿势确认即从单张RGB照片中确定每个关节的(x, y)格式的坐标。</p>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p>​        单人2D姿势确认只需要完成图片中关节位置的检测，而多人2D姿势确认不仅要给出关节的位置，而且要确定关节所属。此外，小且隐约可见的关节、遮挡、距离镜头远近不同的尺寸不同都是多人2D姿势确认要面临的挑战。</p>
<h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>​        多人姿势确认的方法可以分为<strong>自上而下</strong>和<strong>自下而上</strong>两个流派。</p>
<p>​        自上而下的方法首先使用人员检测器给出图片中各个人的边界框，然后在各个人的边界框内采用单人姿势确认器确认各个人的各个部位。</p>
<p>​        自下而上的方法首先检测出图片中所有的部位，然后对属于不同的人的各个部位进行聚合或分组。</p>
<p>​        一般而言，由于自上而下的方法要对检测出的每个边界框进行单人姿势确认，因此随着图片中人的数量上升，自上而下方法需要高昂的计算代价。而自下而上的方法由于损失了全局信息，没有直接使用来自其他身体部分和其他人的全局上下文线索，因此需要采用高消耗的全局推理。</p>
<h3 id="自上而下"><a href="#自上而下" class="headerlink" title="自上而下"></a>自上而下</h3><p>​        由于自上而下的姿势确认方法首先根据特征确认人的边界框，之后再仅根据边界框内的特征给出关节的位置。因此，受到确认的边界框的精确度和尺度的影响。</p>
<p>​        RMPE认为姿势确认的精确度同人员检测框的精确程度高度相关，因此设计出SSTN从不准确的边界框提取出高质量的单人区域，以获得更好的姿势确认结果。</p>
<p>​        Cascaded Pyramid Network for Multi-Person Pose Estimation 发现难处理的关节点无法简单地通过外挂特征辨别，且在训练阶段未做出明确地定位，为此提出了级联金字塔网络结构，用GlobalNet学习基于特征金字塔的良好特征表示，Refinet通过在线难处理关键点的mining loss 处理难处理的关键点。</p>
<p>​        Multi-Person Pose Estimation with Enhanced Channel-wise and Spatial Information 使用注意力机制动态地高级特征和低级特的优点，前者通过更大的视野域处理不可见或者被遮挡的关节，后者拥有更大的分辨率可以帮助确认关节点。</p>
<h3 id="自下而上"><a href="#自下而上" class="headerlink" title="自下而上"></a>自下而上</h3><p>​        自下而上方法关注的重点在于如何对检测出的各个人的部位进行聚类或分组。</p>
<p>​        DeepCut 将各个身体部位分组的问题转换为整数线性规划问题，对各个部位进行标记和聚类。</p>
<p>​        Associate Embedding：End-to-End Learning for Joint Detection and Grouping 将联合聚类方法引入多人姿势确认，首先生成关节热力图和标签热力图，之后将标签相似的关节分组为独立的人。其同传统的自下而上的方法相比，检测阶段和分组阶段是同时进行的，避免了复杂的后续处理。</p>
<p>​        OpenPose：Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields 提出了用PAF，即2D向量场编码肢体的位置和朝向来表示自下而上的聚合分数，并用贪婪算法完成推理节省计算时间。</p>
<p>​    </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/07/10/轨迹追踪问题的可行解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/轨迹追踪问题的可行解/" itemprop="url">轨迹追踪问题的可行解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T15:46:37+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我们规定$m_i^t$ 为时间t位于位置i的物体数量，$f_{i,j}^t$表示时间t时位于位置i，时间t+1时位于位置j的物体数,。根据该定义：</p>
<p>$\forall_{t,j} \sum\limits_{i:j\in N(i)}=m_j^t=\sum\limits_{k\in N(j)}f_{j,k}^t $</p>
<p>对于一般的问题，我们可以做一些假设，例如我们不允许同一个地点存在两个物体。根据这个假设，对于给定的地点，我们可以设定出流上限，即：</p>
<p>$\forall_{k,j}\sum\limits_{j\in N(k)}f_{k,j}^t\leq1$</p>
<p>当然对于入流也是如此。此外，我们还可以知道，</p>
<p>$\forall_{k,j,t}f_{k,j}^t\geq0$</p>
<p>由于跟踪对象会进出跟踪区域，导致系统的总流量发生改变，为此，我们引入了两个特殊节点，$V_{source}$和$V_{sink}$ 与其它的进出节点均连接，除此之外，所有的流在第一帧的时候自$V_{source}$流出，在最后一帧流入$V_{sink}$。这两个节点均为虚拟节点，不表示物理位置，满足：</p>
<p>$ \underbrace{\sum\limits_{j\in N(source)} f_{v_{source},j}^t}= \underbrace{\sum\limits_{j\in N(source)}f_{v_{source} j}^t}$</p>
<p>我们用$M_i^t$ 表示i位置时间一个物体的存在与否，对任一物体，估计存在的后验概率为：</p>
<p>$P_i^t=\hat{P}(M_i^t=1|I^t)$</p>
<p>我们用$m={M_i^t}$表示物体位置的集合。当存在一个流集合满足上面我们定义的关系式，我们称m可行，定义$F$为可行图。则我们的问题转化为了求：</p>
<p>$m^*=arg \max\limits_{m\in F}\hat{P}(M=m|I)$</p>
<p>我们假设$M_i^t$条件独立，这个优化问题可以被重写为:</p>
<p>$m^*=arg \max\limits_{m\in F}log\prod\limits_{t,i}\hat{P}(M_i^t=m_i^t|I^t)$</p>
<p>$=arg \max\limits_{m\in F}\sum\limits_{t,i}log\hat{P}(M_i^t=m_i^t|I^t)$</p>
<p>$=arg \max\limits_{m\in F}\sum\limits_{t,i}(1-m_i^t)log\hat{P}(M_i^t=0|I^t)+m_i^tlog\hat{P}(M_i^t=1|I^t)$</p>
<p>$=arg \max\limits_{m\in F}\sum\limits_{t,i}log\frac{\hat{P}(M_i^t=1|I^t)}{\hat{P}(M_i^t=0|I^t)}$</p>
<p>$=arg \max\limits_{m\in F}\sum\limits_{t,i}(log\frac{p_i^t}{1-p_i^t}m_i^t$</p>
<p>可以转化为线型表达式</p>
<p>$Maximize \sum\limits_{t,i}\log{\frac{p_i^t}{1-p_i^t}}\sum\limits_{j\in N(i)}f_{i,j}$</p>
<p><em>subject to</em> $\forall_{t,i,j},f_i^j\geq0$</p>
<p>$\forall_{t,i},\sum\limits_{j\in N(i)}f_{i,j}^t\leq1$</p>
<p>$\forall_{t,i},\sum\limits_{j\in N(i)}f_{i,j}^t-\sum\limits_{k:i\in N(k)}f_{k,i}^{t-1}\leq0$</p>
<p>$\sum\limits_{j\in N(V_{Source})}f_{v_{source},j}-\sum\limits_{k:V_{sink}\in N(k)}f_{k,v_{sink}}\leq0$</p>
<p>可见我们已经将问题转化为了整数规划问题，但是由于问题规模巨大而且属于NP完全问题，对计算资源要求很高。</p>
<p>这个问题有一个解决途径是转化为有向无环图中的k-最短路径问题。图是有向图且路径是都是节点分离的，两条不同的路径不共享节点且节点简单，且一条路径在途中访问任一节点至多一次。同时，一些边界位置直接与源节点和汇节点相连接。</p>
<p>有向边$e_{i,j}^t$(表示在t时间位于地点i且t+1时间位于地点j的关系)被赋予损失值为$c(e_{i,j}^t)=-log(\frac{p_i^t}{1-p_i^t})$,放射自source node的边设定为0，以便物体自入口随时进入。</p>
<p>(观察损失值可以看出，当概率大于0.5时，损失值为负数，因为我们要求最短路径，因此可以认为其思想为只要该点更可能发生，就让该点发生，同时设置源头点损失值为0是为了防止副作用，即先验情况下P=0.5)</p>
<p>H表示线型表达式的可行解且满足（1）（2）（3）（4）设。</p>
<p>从而我们所需要求得的问题为：</p>
<p>$f^*=\arg\max\limits_{f\in H}\sum\limits_{t,i}c(e_{i,j}^t)\sum\limits_{j\in N(i)}f_{i,j}^t$</p>
<p>令$p_i^<em>$为第i次迭代时的最短路径，$cost(P_l)=\sum\limits_{i=1}^{l}cost(p_i^</em>)$</p>
<p> 找到k条最短路径后,$cost(p_{l+1})\geq cost(p_l)$,从而获得全局最小。</p>
<p>所求的路径即为我们所需的轨迹。</p>
<p>均参考此论文</p>
<p>Multiple Object Tracking  Using K-Shortest Paths Optimization Je ́roˆme Berclaz, Franc ̧ois Fleuret, Engin Tu ̈retken, and Pascal Fua, Senior Member, IEEE </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/07/10/Mac安装BGSlibrary库遇到的小问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/Mac安装BGSlibrary库遇到的小问题/" itemprop="url">Mac安装BGSlibrary库遇到的小问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T15:45:47+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>安装BGSlibrary库的时候遇到了一个错误</p>
<blockquote>
<p>make[2]: <em>*</em> No rule to make target <code>/usr/local/lib/libboost_python3.dylib&#39;, needed by</code>libbgs.dylib’.  Stop.</p>
<p>make[1]: <em>*</em> [CMakeFiles/libbgs.dir/all] Error 2</p>
<p>make: <em>*</em> [all] Error 2</p>
</blockquote>
<p>这个问题解决主要有两个地方需要注意：</p>
<p>第一，如果你是用<code>brew install boost-python3</code> 安装的话，安装的文件在/usr/local/Cellar中的boost文件夹下，需要将其中的文件软链接到/usr/local/lib文件下。</p>
<p>第二，libboost_python3.dylib的文件名可能会根据版本变动，依据我的版本python3.7为例子，你需要加一个软链接 <code>ln -s libboost_python37.dylib libboost_python3.dylib</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/07/06/Camera-calibration-with-OpenCV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/06/Camera-calibration-with-OpenCV/" itemprop="url">Camera calibration with OpenCV</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-06T21:33:12+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>相机有两个因素会导致照片像素失真，分别是：radial factors 和 tangential factors。</p>
<p>为了处理radial factors 采用下面公式为：</p>
<p>$X_{corrected}=x(1+k_1r^2+k_2r^4+k_3r^6)$ </p>
<p>$Y_{corrected} = y(1+k_1r^2+k_2r^4+k_3r^6)$</p>
<p>这种情形体现为“桶状”和”鱼眼“变形。</p>
<p>Tangential 扭曲发生在拍摄镜头没有正对成像平面，它可以通过下面的公式矫正：</p>
<p>$x_{corrected}=x+[2p_1xy+p_2(r^2+2x^2)]$</p>
<p>$y_{corrected}=y+[p_1(r^2+2y^2)+2p_2xy]$</p>
<p>因此OpenCV提出了如下的失真参数矩阵</p>
<p>$Distortion_{coefficients}=(k_1,k_2,p_1,p_2,k_3)$</p>
<p>为了进行坐标转换，我们采用下面的公式</p>
<p>$ \left[ \begin{matrix} x\\y \\w\end{matrix} \right] = \left[ \begin{matrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{matrix} \right]  \left[ \begin{matrix} X \\ Y \\ Z \end{matrix} \right] $</p>
<p> w可由单向性坐标系统表示且w=Z，变量为焦距$f_x$和$f_y$和以像素坐标表示的光学中心。我们用$f_y=f_x*a$的a结合$f=f_x$取代$f_a$和$f_y$.包含$f,a,c_x,c_y$的矩阵称为相机矩阵。</p>
<p>虽然无论使用何种相机分辨率，失真系数都是相同的，但应根据校准分辨率的当前分辨率进行缩放。</p>
<p>因此校准的过程就是确定上面两个矩阵的过程，计算通过基本的几何运算进行。目前OpenCV支持3种物体用来校准：</p>
<ul>
<li>古典的黑白棋盘</li>
<li>对称的圆</li>
<li>不对称的圆</li>
</ul>
<p>更具体，见:<a href="https://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html?" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html?</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/06/18/深度学习中的梯度下降算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/18/深度学习中的梯度下降算法/" itemprop="url">深度学习中的梯度下降算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-18T21:15:16+08:00">
                2018-06-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h2><p>虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。 </p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p><strong>算法 8.1</strong> 随机梯度下降(SGD)在第 k 个训练迭代的更新 </p>
<p><strong>Require:</strong> 学习率 $εk$<br> <strong>Require:</strong> 初始参数 $θ$ </p>
<p>​      <strong>while</strong> 停止准则未满足 <strong>do</strong><br>         从训练集中采包含 $m$ 个样本 {$\mathbf x^{(1)},…,\mathbf x^{(m)}$} 的小批量，其中 $\mathbf x^{(i)}$ 对应目标为 $\mathbf y(i)$。</p>
<p>​         计算梯度估计:$\mathbf g ← + \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)};\mathbf θ), \mathbf y^{(i)})  $    </p>
<p>​        应用更新:$\mathbf θ ← \mathbf θ − ε\mathbf  {g} $</p>
<p>​    <strong>end while</strong> </p>
<h3 id="动量方法"><a href="#动量方法" class="headerlink" title="动量方法"></a>动量方法</h3><p>旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪音的梯度</p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。 </p>
<p>​        计算梯度估计:$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>相对于$\epsilon$,$\alpha$越大，之前梯度对现在方向的影响也越大。</p>
<p>SGD方法中步长只是梯度范数乘以学习率。引入momentum后，步长取决于梯度序列的大小和排列。当很多连续的梯度指向相同的方向时，步长最大。</p>
<h3 id="Nesterov-动量"><a href="#Nesterov-动量" class="headerlink" title="Nesterov 动量"></a>Nesterov 动量</h3><p>Nesterov 动量和标准动量之间的区别体现在梯度计算上。Nesterov 动量中，梯度计算在施加当前速度之后。因此， Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。 </p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。</p>
<p>​        应用临时更新：$\theta \leftarrow\theta+\alpha v$ </p>
<p>​        计算梯度( 在临时点):$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>在凸批量梯度的情况下，Nesterov 动量将额外误差收敛率从 $O(1/k)$($k$ 步后) 改进到 $O(1/k^2)$ ，在随机梯度的情况下，Nesterov 动量没有改进收敛率。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer, required</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">r"""Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Nesterov momentum is based on the formula from</span></span><br><span class="line"><span class="string">    `On the importance of initialization and momentum in deep learning`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float): learning rate</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        The implementation of SGD with Momentum/Nesterov subtly differs from</span></span><br><span class="line"><span class="string">        Sutskever et. al. and implementations in some other frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Considering the specific case of Momentum, the update can be written as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">                  v = \rho * v + g \\</span></span><br><span class="line"><span class="string">                  p = p - lr * v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where p, g, v and :math:`\rho` denote the parameters, gradient,</span></span><br><span class="line"><span class="string">        velocity, and momentum respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This is in contrast to Sutskever et. al. and</span></span><br><span class="line"><span class="string">        other frameworks which employ an update of the form</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">             v = \rho * v + lr * g \\</span></span><br><span class="line"><span class="string">             p = p - v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The Nesterov version is analogously modified.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=required, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, nesterov=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,</span><br><span class="line">                        weight_decay=weight_decay, nesterov=nesterov)</span><br><span class="line">        <span class="keyword">if</span> nesterov <span class="keyword">and</span> (momentum &lt;= <span class="number">0</span> <span class="keyword">or</span> dampening != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Nesterov momentum requires a momentum and zero dampening"</span>)</span><br><span class="line">        super(SGD, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(SGD, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'nesterov'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            weight_decay = group[<span class="string">'weight_decay'</span>]</span><br><span class="line">            momentum = group[<span class="string">'momentum'</span>]</span><br><span class="line">            dampening = group[<span class="string">'dampening'</span>]</span><br><span class="line">            nesterov = group[<span class="string">'nesterov'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> weight_decay != <span class="number">0</span>:</span><br><span class="line">                    d_p.add_(weight_decay, p.data)</span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">'momentum_buffer'</span> <span class="keyword">not</span> <span class="keyword">in</span> param_state:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                        buf.mul_(momentum).add_(d_p)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                        buf.mul_(momentum).add_(<span class="number">1</span> - dampening, d_p)</span><br><span class="line">                    <span class="keyword">if</span> nesterov:</span><br><span class="line">                        d_p = d_p.add(momentum, buf)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        d_p = buf</span><br><span class="line"></span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], d_p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="自适应学习率算法"><a href="#自适应学习率算法" class="headerlink" title="自适应学习率算法"></a>自适应学习率算法</h2><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p><strong>算法 8.4</strong>  AdaGrad 算法<br> <strong>Require:</strong> 全局学习率 $\epsilon$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−7}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← r+g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\delta+\sqrt{r}} ⊙ g$ (逐元素地应用除和求平方根) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<p>AdaGrad 算法独立地适应所有模型参数的学习率，缩放每 个参数反比于其所有梯度历史平方值总和的平方根 。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。 </p>
<p>在凸优化背景中，AdaGrad 算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adagrad</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adagrad algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adaptive Subgradient Methods for Online Learning</span></span><br><span class="line"><span class="string">    and Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        lr_decay (float, optional): learning rate decay (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span></span><br><span class="line"><span class="string">        Optimization: http://jmlr.org/papers/v12/duchi11a.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, lr_decay=lr_decay, weight_decay=weight_decay)</span><br><span class="line">        super(Adagrad, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                state[<span class="string">'sum'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">share_memory</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'sum'</span>].share_memory_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.data.is_sparse:</span><br><span class="line">                        <span class="keyword">raise</span> RuntimeError(<span class="string">"weight_decay option is not compatible with sparse gradients"</span>)</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                clr = group[<span class="string">'lr'</span>] / (<span class="number">1</span> + (state[<span class="string">'step'</span>] - <span class="number">1</span>) * group[<span class="string">'lr_decay'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    grad = grad.coalesce()  <span class="comment"># the update is non-linear so indices must be unique</span></span><br><span class="line">                    grad_indices = grad._indices()</span><br><span class="line">                    grad_values = grad._values()</span><br><span class="line">                    size = grad.size()</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">def</span> <span class="title">make_sparse</span><span class="params">(values)</span>:</span></span><br><span class="line">                        constructor = grad.new</span><br><span class="line">                        <span class="keyword">if</span> grad_indices.dim() == <span class="number">0</span> <span class="keyword">or</span> values.dim() == <span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">return</span> constructor().resize_as_(grad)</span><br><span class="line">                        <span class="keyword">return</span> constructor(grad_indices, values, size)</span><br><span class="line">                    state[<span class="string">'sum'</span>].add_(make_sparse(grad_values.pow(<span class="number">2</span>)))</span><br><span class="line">                    std = state[<span class="string">'sum'</span>]._sparse_mask(grad)</span><br><span class="line">                    std_values = std._values().sqrt_().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.add_(-clr, make_sparse(grad_values / std_values))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)</span><br><span class="line">                    std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.addcdiv_(-clr, grad, std)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><hr>
<p><strong>算法 8.5</strong> RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−6}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\sqrt{\delta+r}} ⊙ g$ ($ \frac{1}{\sqrt{\delta+r}} $逐元素地应用) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<hr>
<p><strong>算法 8.5</strong> 使用Nesterov动量的 RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$,动量系数$\alpha$<br> <strong>Require:</strong> 初始参数 $\theta$, 初始参数$v$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为 y^{(i)}。</p>
<p>​        计算临时更新：$\theta = theta + \alpha v$</p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$v ←\alpha v - \frac{\epsilon}{\sqrt{r}}⊙g$ ($ \frac{1}{\sqrt{r}} $逐元素地应用) </p>
<p>​        应用更新:$\theta ← \theta +v $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSprop</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements RMSprop algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Proposed by G. Hinton in his</span></span><br><span class="line"><span class="string">    `course &lt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The centered version first appears in `Generating Sequences</span></span><br><span class="line"><span class="string">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        alpha (float, optional): smoothing constant (default: 0.99)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span></span><br><span class="line"><span class="string">            the gradient is normalized by an estimation of its variance</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-8</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)</span><br><span class="line">        super(RMSprop, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(RMSprop, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'momentum'</span>, <span class="number">0</span>)</span><br><span class="line">            group.setdefault(<span class="string">'centered'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'RMSprop does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                        state[<span class="string">'grad_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg = state[<span class="string">'square_avg'</span>]</span><br><span class="line">                alpha = group[<span class="string">'alpha'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(alpha).addcmul_(<span class="number">1</span> - alpha, grad, grad)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                    grad_avg = state[<span class="string">'grad_avg'</span>]</span><br><span class="line">                    grad_avg.mul_(alpha).add_(<span class="number">1</span> - alpha, grad)</span><br><span class="line">                    avg = square_avg.addcmul(<span class="number">-1</span>, grad_avg, grad_avg).sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    avg = square_avg.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    buf = state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                    buf.mul_(group[<span class="string">'momentum'</span>]).addcdiv_(grad, avg)</span><br><span class="line">                    p.data.add_(-group[<span class="string">'lr'</span>], buf)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p.data.addcdiv_(-group[<span class="string">'lr'</span>], grad, avg)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>因此，不像 Adam，RMSProp二阶矩估计可能在训练初期有很高的偏置。 Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Torch.optim.Adam(params,lr=0.001,betas=(0.9.0.999),eps=1e-08,weight_decay=0)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adam does not support sparse gradients, please consider SparseAdam instead'</span>)</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg_sq'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">'exp_avg'</span>], state[<span class="string">'exp_avg_sq'</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">'betas'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</span><br><span class="line"></span><br><span class="line">                denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</span><br><span class="line">                step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.data.addcdiv_(-step_size, exp_avg, denom)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adadelta</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adadelta algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        rho (float, optional): coefficient used for computing a running average</span></span><br><span class="line"><span class="string">            of squared gradients (default: 0.9)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-6)</span></span><br><span class="line"><span class="string">        lr (float, optional): coefficient that scale delta before it is applied</span></span><br><span class="line"><span class="string">            to the parameters (default: 1.0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ https://arxiv.org/abs/1212.5701</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1.0</span>, rho=<span class="number">0.9</span>, eps=<span class="number">1e-6</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)</span><br><span class="line">        super(Adadelta, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adadelta does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    state[<span class="string">'acc_delta'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg, acc_delta = state[<span class="string">'square_avg'</span>], state[<span class="string">'acc_delta'</span>]</span><br><span class="line">                rho, eps = group[<span class="string">'rho'</span>], group[<span class="string">'eps'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</span><br><span class="line">                std = square_avg.add(eps).sqrt_()</span><br><span class="line">                delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], delta)</span><br><span class="line">                acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="二阶近似方法"><a href="#二阶近似方法" class="headerlink" title="二阶近似方法"></a>二阶近似方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>除了目标函数的某些特征带来的挑战，如鞍点，牛顿法用于训练大型神经网络还受限于其显著的计算负担。$Hessian$ 矩阵中元素数目是参数数量的平方，因此，如果参数数目为 $k$(甚至是在非常小的神经网络中$ k$ 也可能是百万级别)，牛顿法需要计 算 $k × k$ 矩阵的逆，计算复杂度为 $O(k^3)$。另外，由于参数将每次更新都会改变，每次训练迭代都需要计算 $Hessian$ 矩阵的逆。其结果是，只有参数很少的网络才能在实际中用牛顿法训练。</p>
<p> <strong>算法 8.8</strong> 目标为 $J(θ) = \frac{1}{m} ∑^m_{i=1} L(f(x^{(i)};θ),y^{(i)})$ 的牛顿法 </p>
<p> <strong>Require:</strong> 初始参数 $\theta_0$<br><strong>Require:</strong> 包含 $m$ 个样本的训练集 </p>
<p><strong>while</strong> 没有达到停止准则 <strong>do</strong><br> 计算梯度:$g ←\frac{1}{m}\Delta_\theta \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 矩阵:$H ←  \frac{1}{m}\Delta_\theta^2 \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 逆:$H^{−1}$ </p>
<p> 计算更新:$\Delta\theta = −H^{-1}g $</p>
<p> 应用更新:$\theta = \theta + \Delta\theta </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/21/关于游戏的杂感/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/21/关于游戏的杂感/" itemprop="url">关于游戏的杂感</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-21T18:14:15+08:00">
                2018-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    群里因为两会十名人大代表联名提议抵制游戏炸锅了，作为一个游戏群，可想而知这一提议必然被称为可笑和固步自封的。雷电法王，网瘾战争一再被提及。“不懂教育，甩锅游戏”的论调不绝于耳。“他们根本不是合格的父母”的声音也是不少。但是，就我的所见所闻所感所受，告诉我，似乎他们的步子太大了一点。</p>
<p>​    就我个人所想，这些代表的提议肯定是难以通过的。因为游戏已经获得了他们最初的信徒，这些信徒虽然笃信程度不同，但他们都会为游戏发声。政府想要封禁也是不现实的，毕竟游戏不像真正的毒品，见效如此之快。随着时间的演进，政府也成了游戏的信徒，那么游戏便成为了胜者。人必然是屈服于欲望的。只要给人一个借口，人能向欲望无限屈服。别的不说，看看日本女优的现状便可以知道了。人的堕落是没有下限的。</p>
<p>​    但是现在的游戏真的是什么好东西么？我想手游肯定不是。它蚕食了他们的时间。那些本来应该留给书籍、视频（快手、抖音之流真的与手游之害无异）、外出、亲情的时间杀的一干二净。他们屈服于自己的原始欲望和一些微不足道的成就感。而这必然会导致全社会的人口素质下降。</p>
<p>​    所以我想国家的介入不是坏事。毕竟个人是很难战胜游戏的。因为它就是为了吸引你而生的，它比你更了解你自己。只有一个压迫者存在的情况下，游戏才能健康的成长。否则游戏只会取媚于最广大的一群人，而那群人必然是最低俗的。</p>
<p>​    撸管固然很爽，游戏也是如此。但若是一个社会都撸管，真的不是一件好事。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/13/Kotlin中的lazy属性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/13/Kotlin中的lazy属性/" itemprop="url">Kotlin中的lazy属性</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-13T19:55:27+08:00">
                2018-03-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>学习Kotlin Koans时遇到了一个问题，如下</p>
<blockquote>
<p>Lazy property</p>
<p><em>Add a custom getter to make the ‘lazy’ val really lazy. It should be initialized by the invocation of ‘initializer()’ at the moment of the first access.</em></p>
<p><em>You can add as many additional properties as you need.</em></p>
<p><em>Do not use delegated properties!</em></p>
</blockquote>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer: () -&gt; <span class="built_in">Int</span>) &#123;</span><br><span class="line">     </span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> lazy: <span class="built_in">Int</span></span><br><span class="line">        <span class="keyword">get</span>() &#123;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>语言基础太差，只能去抄答案。答案如下，还是看不懂。</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer:()-&gt;<span class="built_in">Int</span>)&#123;</span><br><span class="line">    <span class="keyword">val</span> value:<span class="built_in">Int</span>?=<span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> lazy:<span class="built_in">Int</span></span><br><span class="line">	    <span class="keyword">get</span>()&#123;</span><br><span class="line">            <span class="keyword">if</span>(value==<span class="literal">null</span>)&#123;</span><br><span class="line">                value=initializer()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> value!!</span><br><span class="line">    	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个其实是对Delegate中的延迟属性(lazy properties)的考察。</p>
<p>所谓的延迟属性即为其值只会在首次访问时使用到的属性。所以上面的问题可以用下面的代码完成，当然暂时忽略其 <em>Do not use delegated properties!</em> 的要求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class LazyProperty(val initializer: () -&gt; Int) &#123;</span><br><span class="line">    val lazyValue: Int by lazy(initializer)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>让我们检查一下上面的代码，其中intializer具有<u>函数类型 () <a href="https://stackoverflow.com/questions/42646016/what-does-the-arrow-operator-do-in-kotlin/42646234" target="_blank" rel="noopener">-&gt;</a> Int</u></p>
<p>所以，其应该是一个不带参数并且返回Int类型值的函数,同时又被LazyProperty类作为参数。由于lazy()是接受一个lambda并返回一个Lazy<t>实际例子的函数，返回的实例可以作为实现延迟属性的委托：第一次调用get()会执行已传递给lazy()的lambda表达式并记录结果，而后续调用get()只是返回记录的结果。</t></p>
<p>再看一次答案</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer: () -&gt; <span class="built_in">Int</span>) &#123;</span><br><span class="line">     <span class="keyword">var</span> value:<span class="built_in">Int</span>? = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> lazy: <span class="built_in">Int</span></span><br><span class="line">        <span class="keyword">get</span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (value==<span class="literal">null</span>)&#123;</span><br><span class="line">                value = initializer()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> value!!</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于 value 的初始值为 null，则 value=initializer() 得以执行，return value结合 get() 使 lazy 获得赋值。由于 value 类型为 Int? 而 lazy 为 Int ,所以需要用 <a href="https://www.kotlincn.net/docs/reference/null-safety.html#-%E6%93%8D%E4%BD%9C%E7%AC%A6" target="_blank" rel="noopener">!!</a> 做非空类型转换。而之后的value均为非空，lambda 不会再执行，从而实现了和 lazy() 一样的效果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/08/总要有第一次/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/08/总要有第一次/" itemprop="url">总要有第一次</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-08T10:56:40+08:00">
                2018-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>没什么好说的</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ICICLE</p>
              <p class="site-description motion-element" itemprop="description">写点东西给自己看</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ICICLE</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=tHMWvb8IDBxhHw0BFkLFO_Apq0c-zmGJ8sb1KXqXMRw&cl=ffffff&w=a"></script>





        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
