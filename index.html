<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="写点东西给自己看">
<meta property="og:type" content="website">
<meta property="og:title" content="呓语">
<meta property="og:url" content="http://icicle314.xyz/index.html">
<meta property="og:site_name" content="呓语">
<meta property="og:description" content="写点东西给自己看">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="呓语">
<meta name="twitter:description" content="写点东西给自己看">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://icicle314.xyz/"/>





  <title>呓语</title>
  








</head>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<script src="https://unpkg.com/kotlin-playground@1" data-selector="highlight kotlin"></script>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">呓语</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">发牢骚的地方</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/06/18/深度学习中的梯度下降算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/18/深度学习中的梯度下降算法/" itemprop="url">深度学习中的梯度下降算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-18T21:15:16+08:00">
                2018-06-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h2><p>虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。 </p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p><strong>算法 8.1</strong> 随机梯度下降(SGD)在第 k 个训练迭代的更新 </p>
<p><strong>Require:</strong> 学习率 $εk$<br> <strong>Require:</strong> 初始参数 $θ$ </p>
<p>​      <strong>while</strong> 停止准则未满足 <strong>do</strong><br>         从训练集中采包含 $m$ 个样本 {$\mathbf x^{(1)},…,\mathbf x^{(m)}$} 的小批量，其中 $\mathbf x^{(i)}$ 对应目标为 $\mathbf y(i)$。</p>
<p>​         计算梯度估计:$\mathbf g ← + \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)};\mathbf θ), \mathbf y^{(i)})  $    </p>
<p>​        应用更新:$\mathbf θ ← \mathbf θ − ε\mathbf  {g} $</p>
<p>​    <strong>end while</strong> </p>
<h3 id="动量方法"><a href="#动量方法" class="headerlink" title="动量方法"></a>动量方法</h3><p>旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪音的梯度</p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。 </p>
<p>​        计算梯度估计:$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>相对于$\epsilon$,$\alpha$越大，之前梯度对现在方向的影响也越大。</p>
<p>SGD方法中步长只是梯度范数乘以学习率。引入momentum后，步长取决于梯度序列的大小和排列。当很多连续的梯度指向相同的方向时，步长最大。</p>
<h3 id="Nesterov-动量"><a href="#Nesterov-动量" class="headerlink" title="Nesterov 动量"></a>Nesterov 动量</h3><p>Nesterov 动量和标准动量之间的区别体现在梯度计算上。Nesterov 动量中，梯度计算在施加当前速度之后。因此， Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。 </p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。</p>
<p>​        应用临时更新：$\theta \leftarrow\theta+\alpha v$ </p>
<p>​        计算梯度( 在临时点):$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>在凸批量梯度的情况下，Nesterov 动量将额外误差收敛率从 $O(1/k)$($k$ 步后) 改进到 $O(1/k^2)$ ，在随机梯度的情况下，Nesterov 动量没有改进收敛率。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer, required</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">r"""Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Nesterov momentum is based on the formula from</span></span><br><span class="line"><span class="string">    `On the importance of initialization and momentum in deep learning`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float): learning rate</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        The implementation of SGD with Momentum/Nesterov subtly differs from</span></span><br><span class="line"><span class="string">        Sutskever et. al. and implementations in some other frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Considering the specific case of Momentum, the update can be written as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">                  v = \rho * v + g \\</span></span><br><span class="line"><span class="string">                  p = p - lr * v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where p, g, v and :math:`\rho` denote the parameters, gradient,</span></span><br><span class="line"><span class="string">        velocity, and momentum respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This is in contrast to Sutskever et. al. and</span></span><br><span class="line"><span class="string">        other frameworks which employ an update of the form</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">             v = \rho * v + lr * g \\</span></span><br><span class="line"><span class="string">             p = p - v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The Nesterov version is analogously modified.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=required, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, nesterov=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,</span><br><span class="line">                        weight_decay=weight_decay, nesterov=nesterov)</span><br><span class="line">        <span class="keyword">if</span> nesterov <span class="keyword">and</span> (momentum &lt;= <span class="number">0</span> <span class="keyword">or</span> dampening != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Nesterov momentum requires a momentum and zero dampening"</span>)</span><br><span class="line">        super(SGD, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(SGD, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'nesterov'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            weight_decay = group[<span class="string">'weight_decay'</span>]</span><br><span class="line">            momentum = group[<span class="string">'momentum'</span>]</span><br><span class="line">            dampening = group[<span class="string">'dampening'</span>]</span><br><span class="line">            nesterov = group[<span class="string">'nesterov'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> weight_decay != <span class="number">0</span>:</span><br><span class="line">                    d_p.add_(weight_decay, p.data)</span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">'momentum_buffer'</span> <span class="keyword">not</span> <span class="keyword">in</span> param_state:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                        buf.mul_(momentum).add_(d_p)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                        buf.mul_(momentum).add_(<span class="number">1</span> - dampening, d_p)</span><br><span class="line">                    <span class="keyword">if</span> nesterov:</span><br><span class="line">                        d_p = d_p.add(momentum, buf)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        d_p = buf</span><br><span class="line"></span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], d_p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="自适应学习率算法"><a href="#自适应学习率算法" class="headerlink" title="自适应学习率算法"></a>自适应学习率算法</h2><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p><strong>算法 8.4</strong>  AdaGrad 算法<br> <strong>Require:</strong> 全局学习率 $\epsilon$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−7}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← r+g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\delta+\sqrt{r}} ⊙ g$ (逐元素地应用除和求平方根) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<p>AdaGrad 算法独立地适应所有模型参数的学习率，缩放每 个参数反比于其所有梯度历史平方值总和的平方根 。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。 </p>
<p>在凸优化背景中，AdaGrad 算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adagrad</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adagrad algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adaptive Subgradient Methods for Online Learning</span></span><br><span class="line"><span class="string">    and Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        lr_decay (float, optional): learning rate decay (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span></span><br><span class="line"><span class="string">        Optimization: http://jmlr.org/papers/v12/duchi11a.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, lr_decay=lr_decay, weight_decay=weight_decay)</span><br><span class="line">        super(Adagrad, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                state[<span class="string">'sum'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">share_memory</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'sum'</span>].share_memory_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.data.is_sparse:</span><br><span class="line">                        <span class="keyword">raise</span> RuntimeError(<span class="string">"weight_decay option is not compatible with sparse gradients"</span>)</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                clr = group[<span class="string">'lr'</span>] / (<span class="number">1</span> + (state[<span class="string">'step'</span>] - <span class="number">1</span>) * group[<span class="string">'lr_decay'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    grad = grad.coalesce()  <span class="comment"># the update is non-linear so indices must be unique</span></span><br><span class="line">                    grad_indices = grad._indices()</span><br><span class="line">                    grad_values = grad._values()</span><br><span class="line">                    size = grad.size()</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">def</span> <span class="title">make_sparse</span><span class="params">(values)</span>:</span></span><br><span class="line">                        constructor = grad.new</span><br><span class="line">                        <span class="keyword">if</span> grad_indices.dim() == <span class="number">0</span> <span class="keyword">or</span> values.dim() == <span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">return</span> constructor().resize_as_(grad)</span><br><span class="line">                        <span class="keyword">return</span> constructor(grad_indices, values, size)</span><br><span class="line">                    state[<span class="string">'sum'</span>].add_(make_sparse(grad_values.pow(<span class="number">2</span>)))</span><br><span class="line">                    std = state[<span class="string">'sum'</span>]._sparse_mask(grad)</span><br><span class="line">                    std_values = std._values().sqrt_().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.add_(-clr, make_sparse(grad_values / std_values))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)</span><br><span class="line">                    std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.addcdiv_(-clr, grad, std)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><hr>
<p><strong>算法 8.5</strong> RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−6}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\sqrt{\delta+r}} ⊙ g$ ($ \frac{1}{\sqrt{\delta+r}} $逐元素地应用) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<hr>
<p><strong>算法 8.5</strong> 使用Nesterov动量的 RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$,动量系数$\alpha$<br> <strong>Require:</strong> 初始参数 $\theta$, 初始参数$v$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为 y^{(i)}。</p>
<p>​        计算临时更新：$\theta = theta + \alpha v$</p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$v ←\alpha v - \frac{\epsilon}{\sqrt{r}}⊙g$ ($ \frac{1}{\sqrt{r}} $逐元素地应用) </p>
<p>​        应用更新:$\theta ← \theta +v $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSprop</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements RMSprop algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Proposed by G. Hinton in his</span></span><br><span class="line"><span class="string">    `course &lt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The centered version first appears in `Generating Sequences</span></span><br><span class="line"><span class="string">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        alpha (float, optional): smoothing constant (default: 0.99)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span></span><br><span class="line"><span class="string">            the gradient is normalized by an estimation of its variance</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-8</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)</span><br><span class="line">        super(RMSprop, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(RMSprop, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'momentum'</span>, <span class="number">0</span>)</span><br><span class="line">            group.setdefault(<span class="string">'centered'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'RMSprop does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                        state[<span class="string">'grad_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg = state[<span class="string">'square_avg'</span>]</span><br><span class="line">                alpha = group[<span class="string">'alpha'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(alpha).addcmul_(<span class="number">1</span> - alpha, grad, grad)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                    grad_avg = state[<span class="string">'grad_avg'</span>]</span><br><span class="line">                    grad_avg.mul_(alpha).add_(<span class="number">1</span> - alpha, grad)</span><br><span class="line">                    avg = square_avg.addcmul(<span class="number">-1</span>, grad_avg, grad_avg).sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    avg = square_avg.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    buf = state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                    buf.mul_(group[<span class="string">'momentum'</span>]).addcdiv_(grad, avg)</span><br><span class="line">                    p.data.add_(-group[<span class="string">'lr'</span>], buf)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p.data.addcdiv_(-group[<span class="string">'lr'</span>], grad, avg)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>因此，不像 Adam，RMSProp二阶矩估计可能在训练初期有很高的偏置。 Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Torch.optim.Adam(params,lr=0.001,betas=(0.9.0.999),eps=1e-08,weight_decay=0)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adam does not support sparse gradients, please consider SparseAdam instead'</span>)</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg_sq'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">'exp_avg'</span>], state[<span class="string">'exp_avg_sq'</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">'betas'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</span><br><span class="line"></span><br><span class="line">                denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</span><br><span class="line">                step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.data.addcdiv_(-step_size, exp_avg, denom)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adadelta</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adadelta algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        rho (float, optional): coefficient used for computing a running average</span></span><br><span class="line"><span class="string">            of squared gradients (default: 0.9)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-6)</span></span><br><span class="line"><span class="string">        lr (float, optional): coefficient that scale delta before it is applied</span></span><br><span class="line"><span class="string">            to the parameters (default: 1.0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ https://arxiv.org/abs/1212.5701</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1.0</span>, rho=<span class="number">0.9</span>, eps=<span class="number">1e-6</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)</span><br><span class="line">        super(Adadelta, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adadelta does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    state[<span class="string">'acc_delta'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg, acc_delta = state[<span class="string">'square_avg'</span>], state[<span class="string">'acc_delta'</span>]</span><br><span class="line">                rho, eps = group[<span class="string">'rho'</span>], group[<span class="string">'eps'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</span><br><span class="line">                std = square_avg.add(eps).sqrt_()</span><br><span class="line">                delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], delta)</span><br><span class="line">                acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="二阶近似方法"><a href="#二阶近似方法" class="headerlink" title="二阶近似方法"></a>二阶近似方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>除了目标函数的某些特征带来的挑战，如鞍点，牛顿法用于训练大型神经网络还受限于其显著的计算负担。$Hessian$ 矩阵中元素数目是参数数量的平方，因此，如果参数数目为 $k$(甚至是在非常小的神经网络中$ k$ 也可能是百万级别)，牛顿法需要计 算 $k × k$ 矩阵的逆，计算复杂度为 $O(k^3)$。另外，由于参数将每次更新都会改变，每次训练迭代都需要计算 $Hessian$ 矩阵的逆。其结果是，只有参数很少的网络才能在实际中用牛顿法训练。</p>
<p> <strong>算法 8.8</strong> 目标为 $J(θ) = \frac{1}{m} ∑^m_{i=1} L(f(x^{(i)};θ),y^{(i)})$ 的牛顿法 </p>
<p> <strong>Require:</strong> 初始参数 $\theta_0$<br><strong>Require:</strong> 包含 $m$ 个样本的训练集 </p>
<p><strong>while</strong> 没有达到停止准则 <strong>do</strong><br> 计算梯度:$g ←\frac{1}{m}\Delta_\theta \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 矩阵:$H ←  \frac{1}{m}\Delta_\theta^2 \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 逆:$H^{−1}$ </p>
<p> 计算更新:$\Delta\theta = −H^{-1}g $</p>
<p> 应用更新:$\theta = \theta + \Delta\theta </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/21/关于游戏的杂感/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/21/关于游戏的杂感/" itemprop="url">关于游戏的杂感</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-21T18:14:15+08:00">
                2018-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    群里因为两会十名人大代表联名提议抵制游戏炸锅了，作为一个游戏群，可想而知这一提议必然被称为可笑和固步自封的。雷电法王，网瘾战争一再被提及。“不懂教育，甩锅游戏”的论调不绝于耳。“他们根本不是合格的父母”的声音也是不少。但是，就我的所见所闻所感所受，告诉我，似乎他们的步子太大了一点。</p>
<p>​    就我个人所想，这些代表的提议肯定是难以通过的。因为游戏已经获得了他们最初的信徒，这些信徒虽然笃信程度不同，但他们都会为游戏发声。政府想要封禁也是不现实的，毕竟游戏不像真正的毒品，见效如此之快。随着时间的演进，政府也成了游戏的信徒，那么游戏便成为了胜者。人必然是屈服于欲望的。只要给人一个借口，人能向欲望无限屈服。别的不说，看看日本女优的现状便可以知道了。人的堕落是没有下限的。</p>
<p>​    但是现在的游戏真的是什么好东西么？我想手游肯定不是。它蚕食了他们的时间。那些本来应该留给书籍、视频（快手、抖音之流真的与手游之害无异）、外出、亲情的时间杀的一干二净。他们屈服于自己的原始欲望和一些微不足道的成就感。而这必然会导致全社会的人口素质下降。</p>
<p>​    所以我想国家的介入不是坏事。毕竟个人是很难战胜游戏的。因为它就是为了吸引你而生的，它比你更了解你自己。只有一个压迫者存在的情况下，游戏才能健康的成长。否则游戏只会取媚于最广大的一群人，而那群人必然是最低俗的。</p>
<p>​    撸管固然很爽，游戏也是如此。但若是一个社会都撸管，真的不是一件好事。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/13/Kotlin中的lazy属性/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/13/Kotlin中的lazy属性/" itemprop="url">Kotlin中的lazy属性</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-13T19:55:27+08:00">
                2018-03-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>学习Kotlin Koans时遇到了一个问题，如下</p>
<blockquote>
<p>Lazy property</p>
<p><em>Add a custom getter to make the ‘lazy’ val really lazy. It should be initialized by the invocation of ‘initializer()’ at the moment of the first access.</em></p>
<p><em>You can add as many additional properties as you need.</em></p>
<p><em>Do not use delegated properties!</em></p>
</blockquote>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer: () -&gt; <span class="built_in">Int</span>) &#123;</span><br><span class="line">     </span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> lazy: <span class="built_in">Int</span></span><br><span class="line">        <span class="keyword">get</span>() &#123;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>语言基础太差，只能去抄答案。答案如下，还是看不懂。</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer:()-&gt;<span class="built_in">Int</span>)&#123;</span><br><span class="line">    <span class="keyword">val</span> value:<span class="built_in">Int</span>?=<span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> lazy:<span class="built_in">Int</span></span><br><span class="line">	    <span class="keyword">get</span>()&#123;</span><br><span class="line">            <span class="keyword">if</span>(value==<span class="literal">null</span>)&#123;</span><br><span class="line">                value=initializer()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> value!!</span><br><span class="line">    	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个其实是对Delegate中的延迟属性(lazy properties)的考察。</p>
<p>所谓的延迟属性即为其值只会在首次访问时使用到的属性。所以上面的问题可以用下面的代码完成，当然暂时忽略其 <em>Do not use delegated properties!</em> 的要求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class LazyProperty(val initializer: () -&gt; Int) &#123;</span><br><span class="line">    val lazyValue: Int by lazy(initializer)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>让我们检查一下上面的代码，其中intializer具有<u>函数类型 () <a href="https://stackoverflow.com/questions/42646016/what-does-the-arrow-operator-do-in-kotlin/42646234" target="_blank" rel="noopener">-&gt;</a> Int</u></p>
<p>所以，其应该是一个不带参数并且返回Int类型值的函数,同时又被LazyProperty类作为参数。由于lazy()是接受一个lambda并返回一个Lazy<t>实际例子的函数，返回的实例可以作为实现延迟属性的委托：第一次调用get()会执行已传递给lazy()的lambda表达式并记录结果，而后续调用get()只是返回记录的结果。</t></p>
<p>再看一次答案</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyProperty</span></span>(<span class="keyword">val</span> initializer: () -&gt; <span class="built_in">Int</span>) &#123;</span><br><span class="line">     <span class="keyword">var</span> value:<span class="built_in">Int</span>? = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> lazy: <span class="built_in">Int</span></span><br><span class="line">        <span class="keyword">get</span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (value==<span class="literal">null</span>)&#123;</span><br><span class="line">                value = initializer()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> value!!</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于 value 的初始值为 null，则 value=initializer() 得以执行，return value结合 get() 使 lazy 获得赋值。由于 value 类型为 Int? 而 lazy 为 Int ,所以需要用 <a href="https://www.kotlincn.net/docs/reference/null-safety.html#-%E6%93%8D%E4%BD%9C%E7%AC%A6" target="_blank" rel="noopener">!!</a> 做非空类型转换。而之后的value均为非空，lambda 不会再执行，从而实现了和 lazy() 一样的效果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/03/08/总要有第一次/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/08/总要有第一次/" itemprop="url">总要有第一次</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-08T10:56:40+08:00">
                2018-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>没什么好说的</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2017/10/28/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/28/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-28T08:39:58+08:00">
                2017-10-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ICICLE</p>
              <p class="site-description motion-element" itemprop="description">写点东西给自己看</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ICICLE</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
