<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="深度学习," />










<meta name="description" content="梯度下降算法基本算法虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。  SGD算法">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中的梯度下降算法">
<meta property="og:url" content="http://icicle314.xyz/2018/06/18/深度学习中的梯度下降算法/index.html">
<meta property="og:site_name" content="呓语">
<meta property="og:description" content="梯度下降算法基本算法虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。  SGD算法">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-06-18T13:33:25.627Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习中的梯度下降算法">
<meta name="twitter:description" content="梯度下降算法基本算法虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。  SGD算法">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://icicle314.xyz/2018/06/18/深度学习中的梯度下降算法/"/>





  <title>深度学习中的梯度下降算法 | 呓语</title>
  








</head>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<script src="https://unpkg.com/kotlin-playground@1" data-selector="highlight kotlin"></script>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">呓语</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">发牢骚的地方</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://icicle314.xyz/2018/06/18/深度学习中的梯度下降算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ICICLE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="呓语">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习中的梯度下降算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-18T21:15:16+08:00">
                2018-06-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h2><p>虽然结果表明，具有自适应学习率(以 RMSProp 和 AdaDelta 为代表)的算法族表现得相当鲁棒，不分伯仲，但没有哪个算法能脱颖而出。 目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。此时，选择哪一个算法似乎主要取决于 使用者对算法的熟悉程度(以便调节超参数)。 </p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p><strong>算法 8.1</strong> 随机梯度下降(SGD)在第 k 个训练迭代的更新 </p>
<p><strong>Require:</strong> 学习率 $εk$<br> <strong>Require:</strong> 初始参数 $θ$ </p>
<p>​      <strong>while</strong> 停止准则未满足 <strong>do</strong><br>         从训练集中采包含 $m$ 个样本 {$\mathbf x^{(1)},…,\mathbf x^{(m)}$} 的小批量，其中 $\mathbf x^{(i)}$ 对应目标为 $\mathbf y(i)$。</p>
<p>​         计算梯度估计:$\mathbf g ← + \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)};\mathbf θ), \mathbf y^{(i)})  $    </p>
<p>​        应用更新:$\mathbf θ ← \mathbf θ − ε\mathbf  {g} $</p>
<p>​    <strong>end while</strong> </p>
<h3 id="动量方法"><a href="#动量方法" class="headerlink" title="动量方法"></a>动量方法</h3><p>旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪音的梯度</p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。 </p>
<p>​        计算梯度估计:$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>相对于$\epsilon$,$\alpha$越大，之前梯度对现在方向的影响也越大。</p>
<p>SGD方法中步长只是梯度范数乘以学习率。引入momentum后，步长取决于梯度序列的大小和排列。当很多连续的梯度指向相同的方向时，步长最大。</p>
<h3 id="Nesterov-动量"><a href="#Nesterov-动量" class="headerlink" title="Nesterov 动量"></a>Nesterov 动量</h3><p>Nesterov 动量和标准动量之间的区别体现在梯度计算上。Nesterov 动量中，梯度计算在施加当前速度之后。因此， Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。 </p>
<p><strong>算法 8.2</strong> 使用动量的随机梯度下降(SGD) </p>
<p><strong>Require:</strong> 学习率 ε，动量参数 $\alpha$</p>
<p> <strong>Require:</strong> 初始参数 $\mathbf θ$，初始速度$ \mathbf v $</p>
<p>​      <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 m 个样本 {$x^{(1)}, . . . , x^{(m)} $}的小批量，对应目标为 $y^{(i)}$。</p>
<p>​        应用临时更新：$\theta \leftarrow\theta+\alpha v$ </p>
<p>​        计算梯度( 在临时点):$\mathbf g ← \frac{1}{m} ∇_\theta ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        计算速度更新:$\mathbf v ← \alpha\mathbf v − ε\mathbf g $</p>
<p>​        应用更新:$\mathbf{\theta}$ ←  $\mathbf\theta$ + v</p>
<p>​     <strong>end while</strong> </p>
<p>在凸批量梯度的情况下，Nesterov 动量将额外误差收敛率从 $O(1/k)$($k$ 步后) 改进到 $O(1/k^2)$ ，在随机梯度的情况下，Nesterov 动量没有改进收敛率。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer, required</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">r"""Implements stochastic gradient descent (optionally with momentum).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Nesterov momentum is based on the formula from</span></span><br><span class="line"><span class="string">    `On the importance of initialization and momentum in deep learning`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float): learning rate</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        dampening (float, optional): dampening for momentum (default: 0)</span></span><br><span class="line"><span class="string">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.zero_grad()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss_fn(model(input), target).backward()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; optimizer.step()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        The implementation of SGD with Momentum/Nesterov subtly differs from</span></span><br><span class="line"><span class="string">        Sutskever et. al. and implementations in some other frameworks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Considering the specific case of Momentum, the update can be written as</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">                  v = \rho * v + g \\</span></span><br><span class="line"><span class="string">                  p = p - lr * v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where p, g, v and :math:`\rho` denote the parameters, gradient,</span></span><br><span class="line"><span class="string">        velocity, and momentum respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This is in contrast to Sutskever et. al. and</span></span><br><span class="line"><span class="string">        other frameworks which employ an update of the form</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">             v = \rho * v + lr * g \\</span></span><br><span class="line"><span class="string">             p = p - v</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The Nesterov version is analogously modified.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=required, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, nesterov=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,</span><br><span class="line">                        weight_decay=weight_decay, nesterov=nesterov)</span><br><span class="line">        <span class="keyword">if</span> nesterov <span class="keyword">and</span> (momentum &lt;= <span class="number">0</span> <span class="keyword">or</span> dampening != <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Nesterov momentum requires a momentum and zero dampening"</span>)</span><br><span class="line">        super(SGD, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(SGD, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'nesterov'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            weight_decay = group[<span class="string">'weight_decay'</span>]</span><br><span class="line">            momentum = group[<span class="string">'momentum'</span>]</span><br><span class="line">            dampening = group[<span class="string">'dampening'</span>]</span><br><span class="line">            nesterov = group[<span class="string">'nesterov'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                d_p = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> weight_decay != <span class="number">0</span>:</span><br><span class="line">                    d_p.add_(weight_decay, p.data)</span><br><span class="line">                <span class="keyword">if</span> momentum != <span class="number">0</span>:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">'momentum_buffer'</span> <span class="keyword">not</span> <span class="keyword">in</span> param_state:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                        buf.mul_(momentum).add_(d_p)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        buf = param_state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                        buf.mul_(momentum).add_(<span class="number">1</span> - dampening, d_p)</span><br><span class="line">                    <span class="keyword">if</span> nesterov:</span><br><span class="line">                        d_p = d_p.add(momentum, buf)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        d_p = buf</span><br><span class="line"></span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], d_p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="自适应学习率算法"><a href="#自适应学习率算法" class="headerlink" title="自适应学习率算法"></a>自适应学习率算法</h2><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p><strong>算法 8.4</strong>  AdaGrad 算法<br> <strong>Require:</strong> 全局学习率 $\epsilon$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−7}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← r+g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\delta+\sqrt{r}} ⊙ g$ (逐元素地应用除和求平方根) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<p>AdaGrad 算法独立地适应所有模型参数的学习率，缩放每 个参数反比于其所有梯度历史平方值总和的平方根 。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。 </p>
<p>在凸优化背景中，AdaGrad 算法具有一些令人满意的理论性质。然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adagrad</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adagrad algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adaptive Subgradient Methods for Online Learning</span></span><br><span class="line"><span class="string">    and Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        lr_decay (float, optional): learning rate decay (default: 0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span></span><br><span class="line"><span class="string">        Optimization: http://jmlr.org/papers/v12/duchi11a.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, lr_decay=lr_decay, weight_decay=weight_decay)</span><br><span class="line">        super(Adagrad, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                state[<span class="string">'sum'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">share_memory</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                state = self.state[p]</span><br><span class="line">                state[<span class="string">'sum'</span>].share_memory_()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> p.grad.data.is_sparse:</span><br><span class="line">                        <span class="keyword">raise</span> RuntimeError(<span class="string">"weight_decay option is not compatible with sparse gradients"</span>)</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                clr = group[<span class="string">'lr'</span>] / (<span class="number">1</span> + (state[<span class="string">'step'</span>] - <span class="number">1</span>) * group[<span class="string">'lr_decay'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    grad = grad.coalesce()  <span class="comment"># the update is non-linear so indices must be unique</span></span><br><span class="line">                    grad_indices = grad._indices()</span><br><span class="line">                    grad_values = grad._values()</span><br><span class="line">                    size = grad.size()</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">def</span> <span class="title">make_sparse</span><span class="params">(values)</span>:</span></span><br><span class="line">                        constructor = grad.new</span><br><span class="line">                        <span class="keyword">if</span> grad_indices.dim() == <span class="number">0</span> <span class="keyword">or</span> values.dim() == <span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">return</span> constructor().resize_as_(grad)</span><br><span class="line">                        <span class="keyword">return</span> constructor(grad_indices, values, size)</span><br><span class="line">                    state[<span class="string">'sum'</span>].add_(make_sparse(grad_values.pow(<span class="number">2</span>)))</span><br><span class="line">                    std = state[<span class="string">'sum'</span>]._sparse_mask(grad)</span><br><span class="line">                    std_values = std._values().sqrt_().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.add_(-clr, make_sparse(grad_values / std_values))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)</span><br><span class="line">                    std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)</span><br><span class="line">                    p.data.addcdiv_(-clr, grad, std)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><hr>
<p><strong>算法 8.5</strong> RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$<br> <strong>Require:</strong> 初始参数 $\theta$<br> <strong>Require:</strong> 小常数 $\delta$，为了数值稳定大约设为 $10^{−6}$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为$ y^{(i)}$。 </p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$\Delta θ ← − \frac{\epsilon}{\sqrt{\delta+r}} ⊙ g$ ($ \frac{1}{\sqrt{\delta+r}} $逐元素地应用) </p>
<p>​        应用更新:$θ ← θ + \Delta θ $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<hr>
<p><strong>算法 8.5</strong> 使用Nesterov动量的 RMSProp 算法</p>
<hr>
<p> <strong>Require:</strong> 全局学习率 $\epsilon$,衰减速率$\rho$,动量系数$\alpha$<br> <strong>Require:</strong> 初始参数 $\theta$, 初始参数$v$</p>
<p>​    初始化梯度累积变量 $r = 0 $</p>
<p>​     <strong>while</strong> 没有达到停止准则 <strong>do</strong> </p>
<p>​        从训练集中采包含 $m$ 个样本 {$x^{(1)}, . . . , x^{(m)}$} 的小批量，对应目标为 y^{(i)}。</p>
<p>​        计算临时更新：$\theta = theta + \alpha v$</p>
<p>​        计算梯度:$g ← \frac{1}{m} ∇_{\theta} ∑_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p>​        累积平方梯度:$r ← \rho r+(1-\rho)g⊙g $</p>
<p>​        计算更新:$v ←\alpha v - \frac{\epsilon}{\sqrt{r}}⊙g$ ($ \frac{1}{\sqrt{r}} $逐元素地应用) </p>
<p>​        应用更新:$\theta ← \theta +v $</p>
<p>​    <strong>end while</strong> </p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSprop</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements RMSprop algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Proposed by G. Hinton in his</span></span><br><span class="line"><span class="string">    `course &lt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The centered version first appears in `Generating Sequences</span></span><br><span class="line"><span class="string">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-2)</span></span><br><span class="line"><span class="string">        momentum (float, optional): momentum factor (default: 0)</span></span><br><span class="line"><span class="string">        alpha (float, optional): smoothing constant (default: 0.99)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span></span><br><span class="line"><span class="string">            the gradient is normalized by an estimation of its variance</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-2</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-8</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=False)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)</span><br><span class="line">        super(RMSprop, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        super(RMSprop, self).__setstate__(state)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            group.setdefault(<span class="string">'momentum'</span>, <span class="number">0</span>)</span><br><span class="line">            group.setdefault(<span class="string">'centered'</span>, <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'RMSprop does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        state[<span class="string">'momentum_buffer'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                        state[<span class="string">'grad_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg = state[<span class="string">'square_avg'</span>]</span><br><span class="line">                alpha = group[<span class="string">'alpha'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(alpha).addcmul_(<span class="number">1</span> - alpha, grad, grad)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'centered'</span>]:</span><br><span class="line">                    grad_avg = state[<span class="string">'grad_avg'</span>]</span><br><span class="line">                    grad_avg.mul_(alpha).add_(<span class="number">1</span> - alpha, grad)</span><br><span class="line">                    avg = square_avg.addcmul(<span class="number">-1</span>, grad_avg, grad_avg).sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    avg = square_avg.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'momentum'</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    buf = state[<span class="string">'momentum_buffer'</span>]</span><br><span class="line">                    buf.mul_(group[<span class="string">'momentum'</span>]).addcdiv_(grad, avg)</span><br><span class="line">                    p.data.add_(-group[<span class="string">'lr'</span>], buf)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p.data.addcdiv_(-group[<span class="string">'lr'</span>], grad, avg)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>因此，不像 Adam，RMSProp二阶矩估计可能在训练初期有很高的偏置。 Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Torch.optim.Adam(params,lr=0.001,betas=(0.9.0.999),eps=1e-08,weight_decay=0)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adam does not support sparse gradients, please consider SparseAdam instead'</span>)</span><br><span class="line"></span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    <span class="comment"># Exponential moving average of gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    <span class="comment"># Exponential moving average of squared gradient values</span></span><br><span class="line">                    state[<span class="string">'exp_avg_sq'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                exp_avg, exp_avg_sq = state[<span class="string">'exp_avg'</span>], state[<span class="string">'exp_avg_sq'</span>]</span><br><span class="line">                beta1, beta2 = group[<span class="string">'betas'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay the first and second moment running average coefficient</span></span><br><span class="line">                exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line">                exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</span><br><span class="line"></span><br><span class="line">                denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line">                bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line">                bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</span><br><span class="line">                step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</span><br><span class="line"></span><br><span class="line">                p.data.addcdiv_(-step_size, exp_avg, denom)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .optimizer <span class="keyword">import</span> Optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adadelta</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">"""Implements Adadelta algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        rho (float, optional): coefficient used for computing a running average</span></span><br><span class="line"><span class="string">            of squared gradients (default: 0.9)</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-6)</span></span><br><span class="line"><span class="string">        lr (float, optional): coefficient that scale delta before it is applied</span></span><br><span class="line"><span class="string">            to the parameters (default: 1.0)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    __ https://arxiv.org/abs/1212.5701</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1.0</span>, rho=<span class="number">0.9</span>, eps=<span class="number">1e-6</span>, weight_decay=<span class="number">0</span>)</span>:</span></span><br><span class="line">        defaults = dict(lr=lr, rho=rho, eps=eps, weight_decay=weight_decay)</span><br><span class="line">        super(Adadelta, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, closure=None)</span>:</span></span><br><span class="line">        <span class="string">"""Performs a single optimization step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            closure (callable, optional): A closure that reevaluates the model</span></span><br><span class="line"><span class="string">                and returns the loss.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loss = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> closure <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">'params'</span>]:</span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                grad = p.grad.data</span><br><span class="line">                <span class="keyword">if</span> grad.is_sparse:</span><br><span class="line">                    <span class="keyword">raise</span> RuntimeError(<span class="string">'Adadelta does not support sparse gradients'</span>)</span><br><span class="line">                state = self.state[p]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># State initialization</span></span><br><span class="line">                <span class="keyword">if</span> len(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">'step'</span>] = <span class="number">0</span></span><br><span class="line">                    state[<span class="string">'square_avg'</span>] = torch.zeros_like(p.data)</span><br><span class="line">                    state[<span class="string">'acc_delta'</span>] = torch.zeros_like(p.data)</span><br><span class="line"></span><br><span class="line">                square_avg, acc_delta = state[<span class="string">'square_avg'</span>], state[<span class="string">'acc_delta'</span>]</span><br><span class="line">                rho, eps = group[<span class="string">'rho'</span>], group[<span class="string">'eps'</span>]</span><br><span class="line"></span><br><span class="line">                state[<span class="string">'step'</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> group[<span class="string">'weight_decay'</span>] != <span class="number">0</span>:</span><br><span class="line">                    grad = grad.add(group[<span class="string">'weight_decay'</span>], p.data)</span><br><span class="line"></span><br><span class="line">                square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</span><br><span class="line">                std = square_avg.add(eps).sqrt_()</span><br><span class="line">                delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</span><br><span class="line">                p.data.add_(-group[<span class="string">'lr'</span>], delta)</span><br><span class="line">                acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="二阶近似方法"><a href="#二阶近似方法" class="headerlink" title="二阶近似方法"></a>二阶近似方法</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>除了目标函数的某些特征带来的挑战，如鞍点，牛顿法用于训练大型神经网络还受限于其显著的计算负担。$Hessian$ 矩阵中元素数目是参数数量的平方，因此，如果参数数目为 $k$(甚至是在非常小的神经网络中$ k$ 也可能是百万级别)，牛顿法需要计 算 $k × k$ 矩阵的逆，计算复杂度为 $O(k^3)$。另外，由于参数将每次更新都会改变，每次训练迭代都需要计算 $Hessian$ 矩阵的逆。其结果是，只有参数很少的网络才能在实际中用牛顿法训练。</p>
<p> <strong>算法 8.8</strong> 目标为 $J(θ) = \frac{1}{m} ∑^m_{i=1} L(f(x^{(i)};θ),y^{(i)})$ 的牛顿法 </p>
<p> <strong>Require:</strong> 初始参数 $\theta_0$<br><strong>Require:</strong> 包含 $m$ 个样本的训练集 </p>
<p><strong>while</strong> 没有达到停止准则 <strong>do</strong><br> 计算梯度:$g ←\frac{1}{m}\Delta_\theta \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 矩阵:$H ←  \frac{1}{m}\Delta_\theta^2 \sum_i L(f(x^{(i)}; θ), y^{(i)}) $</p>
<p> 计算 $Hessian$ 逆:$H^{−1}$ </p>
<p> 计算更新:$\Delta\theta = −H^{-1}g $</p>
<p> 应用更新:$\theta = \theta + \Delta\theta </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/21/关于游戏的杂感/" rel="next" title="关于游戏的杂感">
                <i class="fa fa-chevron-left"></i> 关于游戏的杂感
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/06/Camera-calibration-with-OpenCV/" rel="prev" title="Camera calibration with OpenCV">
                Camera calibration with OpenCV <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ICICLE</p>
              <p class="site-description motion-element" itemprop="description">写点东西给自己看</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降算法"><span class="nav-number">1.</span> <span class="nav-text">梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本算法"><span class="nav-number">1.1.</span> <span class="nav-text">基本算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SGD"><span class="nav-number">1.1.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动量方法"><span class="nav-number">1.1.2.</span> <span class="nav-text">动量方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Nesterov-动量"><span class="nav-number">1.1.3.</span> <span class="nav-text">Nesterov 动量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自适应学习率算法"><span class="nav-number">1.2.</span> <span class="nav-text">自适应学习率算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaGrad"><span class="nav-number">1.2.1.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSProp"><span class="nav-number">1.2.2.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">1.2.3.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adadelta"><span class="nav-number">1.2.4.</span> <span class="nav-text">Adadelta</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二阶近似方法"><span class="nav-number">1.3.</span> <span class="nav-text">二阶近似方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#牛顿法"><span class="nav-number">1.3.1.</span> <span class="nav-text">牛顿法</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ICICLE</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
